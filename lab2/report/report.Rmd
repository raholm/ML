---
title: "Introduction to Machine Learning"
subtitle: "Lab 2"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
knitr::read_chunk("../assignment2/solution.R")
```

\newpage

# Assignment 1
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

In this assignment I have used the swiss data set containing information about fertility in various provinces of Switzerland at about 1888, 47 observations in total. The features are

\begin{itemize}
\item Fertility: common standardizedfertility measure
\item Agriculture: \% of males involved in agriculture as occupation
\item Examination: \% draftees receiving highest mark on army examination
\item Education: \% education beyond primary school for draftees
\item Catholic: \% ‘catholic’ (as opposed to ‘protestant’)
\item Infant Mortality: live births who live less than 1 year
\end{itemize}

and I am interested in predicting the fertility.

## 2
In order to find the best set of features in terms of sum of squared error (SSE) for linear regression I used cross-validation with 5 folds and averaged the SSE over the folds. The data have been scaled because of different metrics and figure \ref{fig:SSE} shows that a model with 4 features yield the least average SSE out of all models.

```{r SSE, echo=FALSE, eval=TRUE, fig.cap="SSE"}
<<assign1-plot-sse>>
```

With the knowledge gained from cross-validation I trained the final linear regression model on the complete data set. The model resulted in

\begin{equation*}
\begin{aligned}
\hat{y} = & \text{ } `r coefs[1]` + `r features[1]` * `r coefs[2]` \\
& + `r features[2]` * `r  coefs[3]` + `r features[3]` * `r coefs[4]`\\
& +  `r features[4]` * `r coefs[5]`.
\end{aligned}
\end{equation*}

We can see that education decreases the fertility measure which is unsurprising and is present in today's world where industrial countries have in general lower fertility rate compared to developing countries. That infant mortality increases the fertility measure is not surprising either, survival is the key as with all species of animals and that means reproduction and if children die more have to be born in order to balance it out. Agriculture and Catholic variables have lower coefficients but higher values in their measurement so they are still important, but it is not obvious why they are important. A lot of males occupied in agriculture probably means that food is available and it is therefore less requiring for extra sons to be born to provide for the family. A population with big portion being catholic are perhaps more happy in general and have healthier relationships which results in more children.

\newpage

# Assignment 2
```{r, echo=FALSE, eval=TRUE}
<<assign2-init>>
```

In this assignment I have used the tecator data set that contains information used to investigate if infrared absorbance spectrum can be used to predict the fat content of samples of meat. For each meat sample the data consists of a 100 channel spectrum of absorbance records and the levels of moisture (water), fat and protein.

## 1
Figure \ref{fig:2} clearly shows a linear correlation between moisture and protein with a few observations deviating from the overall pattern. A linear model should therefore fit the data well.

```{r 2, echo=FALSE, eval=TRUE, fig.cap="Moisture against Protein."}
<<assign2-2-plot>>
```

## 2
In this polynomial model we have that the target is $y = \textit{Moisture}$, the response is $x = \textit{Protein}$, and we assume that

\begin{equation*}
y \sim \mathcal{N}(\mu, \sigma^{2}).
\end{equation*}

We know from the law of large numbers that the mean can be approximated by

\begin{equation*}
\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
\end{equation*}

and the mean squared error (MSE) of an estimator $\hat{\mu}$ is

\begin{equation*}
\text{MSE}(\hat{\mu}) = \text{var}(\hat{\mu}) + \left[ \text{bias}(\hat{\mu}) \right]^{2}.
\end{equation*}

It turns out that

\begin{equation*}
\text{MSE}(\hat{\mu}) = \text{E}\left[ (\hat{\mu} - \mu)^{2} \right] = \frac{\sigma^{2}}{n}
\end{equation*}

is the best unbiased estimator for a Gaussian distribution, i.e. having the lowest MSE out of all unbiased estimators. That is the reason why it is reasonable to use the MSE to fit our model to the training data. The model will look like

\begin{equation*}
\text{E}[y(x, w)] = \sum_{i=0}^{p} w_{i} \phi_{i}(x)
\end{equation*}

where $\phi_{i}(x) = x^{i}$.

## 3
To fit different polynomial models where \textit{Moisture} is the target and \textit{Protein} is the response, I created two data sets, train and test, of equal size. I fit models up to 6 polynomial terms and figure \ref{fig:3} shows how the train/test mean squared error (MSE) changed with increasing terms. We can see that using only 1 term yield the best test MSE and then is starts to overfit the training data as more polynomial terms are added to the model, i.e. the variance increases. In this case the linear model is the optimal in terms of test MSE and it has a high bias but low variance. The result follows from figure \ref{fig:2}.

```{r 3, echo=FALSE, eval=TRUE, fig.cap="How the mean squared error changes with increasing polynomial terms."}
<<assign2-3>>
```

In the following exercises I will fit linear models where \textit{Fat} is the target and the \textit{channels} are the response variables (100 of those).

## 4
```{r 4, echo=FALSE, eval=TRUE}
<<assign2-4>>
```
Using Akaike information criterion (AIC) in both directions the optimal model contains `r AIC_feature_selection_count` response variables, i.e. `r 100 - AIC_feature_selection_count` were found redundant.

## 5
Here I have used ridge regression and from figure \ref{fig:5} we can see that the coefficient values decay towards 0 as $\lambda$ increases as expected from the regularization term.

```{r 5, echo=FALSE, eval=TRUE, fig.cap="How the coefficients in the model changes with increasing lambda."}
<<assign2-5>>
```

## 6
Here I have used lasso regression and from figure \ref{fig:6} we can see that the coefficient values becomes exactly 0 as lambda increases as expected from the regularization term. The coefficient values goes toward 0 in ridge regression while lasso regression sets the values to exactly 0, i.e. removes coefficients completely.

```{r 6, echo=FALSE, eval=TRUE, fig.cap="How the coefficients in the model changes with increasing lambda."}
<<assign2-6>>
```

## 7
```{r, echo=FALSE, eval=TRUE}
<<assign2-7>>
```

Here I have used 10-folds cross-validation to find the optimal $\lambda$ for lasso regression and the optimal $\lambda$ found was `r lasso_optimal_lambda` which resulted in only `r lasso_feature_selection_count` features with non-zero values excluding the intercept. Figure \ref{fig:7} shows how the cross-validation score varies with $\lambda$ and we can see that it increases as $\lambda$ increases. This indicates that the model starts to underfit the data with greater values of $\lambda$, i.e. too many coefficients get a value of 0.

```{r 7, echo=FALSE, eval=TRUE, fig.cap="How the mean squared error changes with lambda."}
<<assign2-7-plot>>
```

## 8
By using lasso regression, the optimal number of features are significantly less than found by AIC (`r lasso_feature_selection_count` compared to `r AIC_feature_selection_count`). This indicates that lasso regression penalize complex models more harshly compared to AIC.

\newpage

# Appendix

## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>
<<assign1-plot-sse>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2-init>>
<<assign2-2-plot>>
<<assign2-3>>
<<assign2-4>>
<<assign2-5>>
<<assign2-6>>
<<assign2-7>>
<<assign2-7-plot>>
```
