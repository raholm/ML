---
title: "Introduction to Machine Learning"
subtitle: "Lab 1"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
knitr::read_chunk("../assignment2/solution.R")
```

\newpage

\section{Assignment 1}
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

In this assignment I have used the spambase data set that contains word frequencies for mails classified as either spam or non-spam. The data set contains 2740 observations.
In order to test the $k$-nearest neighbor algorithm, I have separated the data set into two sets, training and test sets, of equal size. In the first couple of exercises I have used a threshold = 0.5 and the cosine distance to compute dissimilarities.

## 1.3
Here I used the training set to predict the test set using $k=5$ and the resulting confusion matrix can be seen below.

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-testtable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{242 + 193}{695 + 242 + 193 + 240} = \frac{87}{274} \approx 31.8\%$.
\end{center}

And below is the result from predicting the training data.


```{r, echo=FALSE, eval=TRUE}
<<assign1-3-traintable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{158 + 119}{787 + 158 + 119 + 306} = \frac{277}{1370} \approx 20.2\%.$
\end{center}

## 1.4
Below are the same results as previously but by setting $k$ = 1. First is the result from predicting the test set and the other is from predicting the training set.

```{r, echo=FALSE, eval=TRUE}
<<assign1-4-testtable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{298 + 178}{639 + 298 + 178 + 255} = \frac{238}{685} \approx 34.7\%.$
\end{center}

The performance have degraded compared to $k = 5$ which is not suprising since using $k = 1$ results in  a more complex model since the number of effective parameters are $\frac{\text{\# of training samples}}{k}$ and it overfits the training data.

```{r, echo=FALSE, eval=TRUE}
<<assign1-4-traintable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{6 + 2}{939 + 6 + 2 + 423} = \frac{4}{685} \approx 0.6\%.$
\end{center}

As expected, the number of errors are very low but I would have even expected 0 errors. Maybe the errors are due to numerical precision, but we can at least see that using $k = 1$ entail in very low error rate compared to $k = 5$ on the training data.

## 1.5
Here I have used algorithm from the kknn package with $k = 5$ and used the Euclidean distance instead of the cosine distance. Below is the result from classifying the test set.

```{r, echo=FALSE, eval=TRUE}
<<assign1-5-table>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{291 + 186}{646 + 291 + 186 + 247} = \frac{477}{1370} \approx 34.8\%.$
\end{center}

The misclassification rate is very similar to the one using $k = 1$ with cosine distance and a few percentage points above $k = 5$ with cosine distance. This indicates that the Euclidean distance is not a particular good dissimilarity measure in this case and the cosine distance should be favored which further support why the cosine similarity/distance is widely used in text mining.

## 1.6
```{r, echo=FALSE, eval=TRUE}
<<assign1-6-init>>
```

In order to compare the performance of the classifiers and how it changes with the threshold, I have fixed $k = 5$ and classified the test data with both cosine and Euclidean distance. The threshold that have been used are $0.05, 0.1, \ldots, 0.95$ and figure \ref{fig:ROC} shows the performance comparison using ROC curves. As the false positive rate increases, the threshold increases.

```{r ROC, echo=FALSE, fig.cap="Receiver operating characteristic curves."}
<<assign1-6-ROC>>
```

We can see that using cosine distance, i.e. the red curve, yield in general better performance than Euclidean distance, the green curve. The point on the red curve at around 0.25 false positive rate have a threshold $\in [0.45, 0.6]$ which contains $0.5$. I would expect it to be the optimal threshold and the ROC curves support that.

\newpage

\section{Assignment 2}
In this assignment I have used the machine data set that contains information about the lifetime of machines. The data set contains 48 observations.

```{r, echo=FALSE, eval=TRUE}
<<assign2-init>>
```

## 2.2
```{r, echo=FALSE, eval=TRUE}
<<assign2-2-init>>
```

From the given formula the lifetime of the machines is an exponential distribution and figure \ref{fig:distribution} further supports that.

```{r distribution, echo=FALSE, fig.cap="The distribution of the machine data set."}
<<assign2-2-plot-distribution>>
```

To compute the maximum likelihood estimate, the following formula is used

\begin{equation*}
L(\theta) = f(x | \theta) = \prod_{i = 1}^{m}{f(x_{i} | \theta)}
\end{equation*}

and it is equivalent to maximize the log-likelihood which is given by

\begin{equation*}
l(\theta) = \log{L(\theta)} = \sum_{i = 1}^{m}{\log{f(x_{i} | \theta)}}.
\end{equation*}

Figure \ref{fig:likelihood2} shows the log-likelihood over various $\theta$ and the maximum likelihood estimate is given by $\theta \in [ 1.0, 1.2 ]$.

```{r likelihood2, echo=FALSE, fig.cap="The log-likelihoods over various thetas using the complete data set."}
<<assign2-2-plot-likelihoods>>
```

## 2.3
```{r, echo=FALSE, eval=TRUE}
<<assign2-3-init>>
```

Figure \ref{fig:likelihood3} shows the log-likelihoods when using the complete data set but also when using only 6 observations. Figure \ref{fig:likelihood36} shows the log-likelihoods using 6 observations on its own and the maximum $\theta \in [1.5, 2]$ which is not the same as found using the complete data set. However, using the complete data set is more reliable because we do not lose any available information which cannot be said when picking just a handful of observations and the choice of those observations have a large impact on the result. We should therefore be more confident in the estimate from the complete data set, i.e. $\theta \in [ 1.0, 1.2 ]$.

```{r likelihood3, echo=FALSE, fig.cap="The log-likelihoods over various thetas. Orange line uses the complete data set while the blue line uses only 6 data points.", fig.width=4.5, fig.height=3.5}
<<assign2-3-plot-likelihoods>>
```

```{r likelihood36, echo=FALSE, fig.cap="The log-likelihoods over various thetas using only 6 observations.", fig.width=4.5, fig.height=3.5}
<<assign2-3-plot-likelihoods-6>>
```

## 2.4
```{r, echo=FALSE, eval=TRUE}
<<assign2-4-init>>
```

Here I have computed the log-posterior using all the observations and figure \ref{fig:posterior} shows the result for various $\theta$. The optimal $\theta \in [ 0.8, 1.0 ]$ which is close to the one found previously by the maximum log-likelihood estimate.

```{r posterior, echo=FALSE, fig.cap="The log-posterior oover various thetas using all observations."}
<<assign2-4-plot-posteriors>>
```

## 2.5
```{r, echo=FALSE, eval=TRUE}
<<assign2-5-init>>
```

I used the  maximum log-likelihood estimate $\theta_{\text{MLE}} = 1.1$ and generated a new data set consisting of 50 observations. Figure \ref{fig:distribution5} shows the machine data set and the generated data as histograms and we can see both have similar distributions as expected.

```{r distribution5, echo=FALSE, fig.cap="Left: Histogram of the generated data. Right: Histogram  of the machine data."}
<<assign2-5-plot-distribution>>
```

\newpage

# Appendix

## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>
<<assign1-3-testtable>>
<<assign1-3-traintable>>
<<assign1-4-testtable>>
<<assign1-4-traintable>>
<<assign1-5-table>>
<<assign1-6-init>>
<<assign1-6-ROC>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2-2-init>>
<<assign2-2-plot-distribution>>
<<assign2-2-plot-likelihoods>>
<<assign2-3-init>>
<<assign2-3-plot-likelihoods>>
<<assign2-3-plot-likelihoods-6>>
<<assign2-4-init>>
<<assign2-4-plot-posteriors>>
<<assign2-5-init>>
<<assign2-5-plot-distribution>>
```
