---
title: "Introduction to Machine Learning"
subtitle: "Lab 1"
author: "Anton Persson, Emil Klasson Svensson, Mattias Karlsson, Rasmus Holm"
date: "`r Sys.Date()`"
output:
    pdf_document:
        toc: true
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
```

\newpage

\section{Assignment 1}
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

In this assignment we have used the spambase data set that contains word frequencies for mails classified as either spam or non-spam. The data set contains 2740 observations.
In order to test the $k$-nearest neighbor algorithm, we have separated the data set into two sets, training and test sets, of equal size. In the first couple of exercises we have set the threshold = 0.5 and the cosine distance to compute dissimilarities.

## 1.3
Here we used the training set to predict the test set using $k=5$ and the resulting confusion matrix can be seen below.

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-testtable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{242 + 193}{695 + 242 + 193 + 240} = \frac{87}{274} \approx 31.8\%$.
\end{center}

And below is the result from predicting the training data.


```{r, echo=FALSE, eval=TRUE}
<<assign1-3-traintable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{158 + 119}{787 + 158 + 119 + 306} = \frac{277}{1370} \approx 20.2\%.$
\end{center}

## 1.4
Below are the same results as previously but by setting $k$ = 1. First is the result from predicting the test set and the other is from predicting the training set.

```{r, echo=FALSE, eval=TRUE}
<<assign1-4-testtable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{298 + 178}{639 + 298 + 178 + 255} = \frac{238}{685} \approx 34.7\%.$
\end{center}

The performance have degraded compared to $k = 5$ which is not suprising since using $k = 1$ results in  a more complex model since the number of effective parameters are $\frac{\text{\# of training samples}}{k}$ and it overfits the training data.

```{r, echo=FALSE, eval=TRUE}
<<assign1-4-traintable>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{6 + 2}{939 + 6 + 2 + 423} = \frac{4}{685} \approx 0.6\%.$
\end{center}

As expected, the number of errors are very low but unexpectedly not 0 errors. This has probably to do with numerical precision, but we can at least see that using $k = 1$ entail in very low error rate compared to $k = 5$ on the training data.

## 1.5
Here we used the $k$-nearest neighbor algorithm from the kknn package with $k = 5$ and used the Euclidean distance instead of the cosine distance. Below is the result from classifying the test set.

```{r, echo=FALSE, eval=TRUE}
<<assign1-5-table>>
```

The misclassification rate is calculated as

\begin{center}
$\frac{291 + 186}{646 + 291 + 186 + 247} = \frac{477}{1370} \approx 34.8\%.$
\end{center}

The misclassification rate is very similar to the one using $k = 1$ with cosine distance and a few percentage points above $k = 5$ with cosine distance. This indicates that the Euclidean distance is not a particular good dissimilarity measure in this case and the cosine distance should be favored which further support why the cosine similarity/distance is widely used in text mining.

## 1.6
```{r, echo=FALSE, eval=TRUE}
<<assign1-6-init>>
```

In order to compare the performance of the classifiers and how it changes with the threshold, we have fixed $k = 5$ and classified the test data with both cosine and Euclidean distance. The threshold that have been used are $0.05, 0.1, \ldots, 0.95$ and figure \ref{fig:ROC} shows the performance comparison using ROC curves. As the false positive rate increases, the threshold increases.

```{r ROC, echo=FALSE, fig.cap="Receiver operating characteristic curves."}
<<assign1-6-ROC>>
```

We can see that using cosine distance, i.e. the red curve, yield in general better performance than Euclidean distance, the green curve. The point on the red curve at around 0.25 false positive rate have a threshold $\in [0.45, 0.6]$ which contains $0.5$. We would expect it to be the optimal threshold given that the errors, FPs and FNs, have equal cost and the ROC curves support that.

\newpage

## Assignment 2

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
machines <- data.frame(read.csv("../data/machines.csv"))
machines <- as.vector(unlist(machines$Length))
```

### 2.2 

#### What is the distrubution type of x?
The distrubution given is a exponential distrubution. 

#### Write a function that computes the log-likelihood for a given $\theta$ and data vector x 

The log-likelihood was computed to $log\,p(x|\theta)=n\,log(\theta)-\theta\sum_{n=1}^N\mathrm{x_i}$

```{r, echo=FALSE}
explog <-function(theta,data){
  return(length(data)*log(theta,base = exp(1)) - (theta * sum(data))) 
}
```

#### Plot the curve showing the dependence of log-likelihood on $\theta$ where the entire data is used for fitting.


```{r, echo=FALSE}
theta <- seq(0.001, 5, by=0.01) #Generates a sequence of different 
thetaML <- sapply( X = theta, FUN = explog, data = machines) 
#apply the different thetas to the log-likelihood with the machine data-set.
plotData <- data.frame(x=theta,y=thetaML) #Put the data in a frame. 

library(ggplot2)
#Plotting

p <- ggplot() + geom_line(data=plotData,aes(x=x,y=y,col="All observations")) +
                labs(title=paste("Log-likelihood function over different  theta"),
       x=expression(theta),y="ML-estimate")
plot(p)
```

The maximum log-likelihood is estimated to about 1.1 according to the plot. 

\newpage

## 2.3 

```{r, echo=FALSE}
thetaML3 <- sapply( X = theta, FUN = explog, data = machines[1:6])
#Using only the 6 first observations of machines 
plotData$y2 <- thetaML3

p + geom_line(data = plotData, aes(x=x,y=y2,col="6 observations"))

```

The light red line shows the likelihood function of different theta using six observations and the light blue line shows the estimate using all observations. Both lines increases at the starting values of theta and reaches their peaks at about the same time. The light red line has a higher intercept than the light blue curve, they differ with about 300 units. 

The difference is that the estimate using all observations dies down at a much faster rate than the one using six observations and displays a much clearer max-value than the one with six observations. This shows a large dependence of data that the Maximum Likelihood estimators use. 

\newpage

## 2.4

```{r, echo=FALSE}

postBayes <- function(data,theta){ 
  lambda <- 10 
  return(length(data) * log(theta) - theta * sum(data) +
           log(lambda) - (lambda*theta))
  
} 

thetaBayes <- sapply(theta,FUN = postBayes,data = machines)

plotDataBayes <- data.frame(x=theta,y=thetaBayes)
B <- ggplot() + geom_line(data=plotDataBayes,aes(x=x,y=y)) +
  labs(title="Bayes-estimation of exponential distrubution",
       x=expression(theta),y="Bayes-estimate")
plot(B)

```

\newpage
## 2.5 

```{r, echo=FALSE}
library(gridExtra)


#extract the exact theta for the  ML-estimate
maxTheta <- theta[thetaML == max(thetaML)] 
# Generate 50 random variables from the exponential distrubution with
# the ML-estimate extracted  
set.seed(12345)
randomExpData<- data.frame(x=rexp(50,rate = maxTheta))

#Plotting
ab<- ggplot(data = randomExpData,aes(x=x)) + geom_histogram(bins = 10)
abc<- ggplot(data =data.frame(x=machines),aes(x=x)) + geom_histogram(bins = 10)

 grid.arrange( ab, abc, ncol=2)

```

The distribution over lifetime of machines have similar properties as the sampled observations from the exponential distribution with the ML-estimate of theta. One observation  departs in the machine data set, it has a value above 4 wich is a considerably bigger then the rest.

\newpage

# Appendix

## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>
<<assign1-3-testtable>>
<<assign1-3-traintable>>
<<assign1-4-testtable>>
<<assign1-4-traintable>>
<<assign1-5-table>>
<<assign1-6-init>>
<<assign1-6-ROC>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
machines <- data.frame(read.csv("../data/machines.csv"))
machines <- as.vector(unlist(machines$Length))
explog <-function(theta,data){
  return(length(data)*log(theta,base = exp(1)) - (theta * sum(data))) 
}
theta <- seq(0.001, 5, by=0.01) #Generates a sequence of different 
thetaML <- sapply( X = theta, FUN = explog, data = machines) 
#apply the different thetas to the log-likelihood with the machine data-set.
plotData <- data.frame(x=theta,y=thetaML) #Put the data in a frame. 

library(ggplot2)
#Plotting

p <- ggplot() + geom_line(data=plotData,aes(x=x,y=y,col="All observations")) +
                labs(title=paste("Log-likelihood function over different  theta"),
       x=expression(theta),y="ML-estimate")
plot(p)
thetaML3 <- sapply( X = theta, FUN = explog, data = machines[1:6])
#Using only the 6 first observations of machines 
plotData$y2 <- thetaML3

p + geom_line(data = plotData, aes(x=x,y=y2,col="6 observations"))

postBayes <- function(data,theta){ 
  lambda <- 10 
  return(length(data) * log(theta) - theta * sum(data) +
           log(lambda) - (lambda*theta))
  
} 

thetaBayes <- sapply(theta,FUN = postBayes,data = machines)

plotDataBayes <- data.frame(x=theta,y=thetaBayes)
B <- ggplot() + geom_line(data=plotDataBayes,aes(x=x,y=y)) +
  labs(title="Bayes-estimation of exponential distrubution",
       x=expression(theta),y="Bayes-estimate")
plot(B)
library(gridExtra)


#extract the exact theta for the  ML-estimate
maxTheta <- theta[thetaML == max(thetaML)] 
# Generate 50 random variables from the exponential distrubution with
# the ML-estimate extracted  
set.seed(12345)
randomExpData<- data.frame(x=rexp(50,rate = maxTheta))

#Plotting
ab<- ggplot(data = randomExpData,aes(x=x)) + geom_histogram(bins = 10)
abc<- ggplot(data =data.frame(x=machines),aes(x=x)) + geom_histogram(bins = 10)

grid.arrange( ab, abc, ncol=2)
```
