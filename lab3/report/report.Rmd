---
title: "Introduction to Machine Learning"
subtitle: "Lab 3"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
knitr::read_chunk("../assignment2/solution.R")
```

\newpage

# Assignment 1
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

## 1
```{r plot11, echo=FALSE, eval=TRUE, fig.cap="Carapace length versus rear width."}
<<assign1-1>>
```

Figure \ref{fig:plot11} shows that the crabs could be separated pretty well based on sex by a linear model. With the proper covariance matrices the groups could be seen as Gaussian distributed and therefore would the linear discriminant analysis (LDA) be a reasonable classifier in this case.

## 2
```{r plot12, echo=FALSE, eval=TRUE, fig.cap="Classification by LDA including the decision boundary."}
<<assign1-2>>
```

The LDA classifier does work pretty well in this case as shown in figure \ref{fig:plot12} with a few misclassifications, 7 out of 200 observations to be precise.

## 3
```{r plot13, echo=FALSE, eval=TRUE, fig.cap="The true data with the LDA decision boundary."}
<<assign1-3>>
```

The true data can be seen in figure \ref{fig:plot13} together with the decision boundary by LDA. We can see that the overall fit is very good but it has difficulties in the left part of the plot where observations are very close to the decision boundary.

## 4
```{r plot14, echo=FALSE, eval=TRUE, warning=FALSE, fig.cap="Classification by logistic regression including the decision boundary."}
<<assign1-4>>
```

Here I have used logistic regression instead of LDA to classify based on sex. Figure \ref{fig:plot14} shows a similar result that came from LDA in figure \ref{fig:plot12}. The number of miscalssifications are 7 as well but it is rather difficult to know exactly which one is preferred in this case. Logistic regression doesn't assume the data to be normally distributed which LDA does and since that assumption is not always true I would probably prefer logistic regression in this case.

\newpage

# Assignment 2
```{r, echo=FALSE, eval=TRUE}
<<assign2-init>>
<<assign2-1>>
```

In order to do the following exercises I have divided the creditscoring data set into three sets; train (50\%), validation (25\%), and test (25\%). The variable good/bad indicates how the customers have managed their loans and I will be using decision trees and naive Bayes classifiers to predict the aforementioned variable using all other variables.

## 2
Here I am interested in whether the deviance or Gini index as impurity measurement give rise to the lowest misclassification rates in decision trees. I have trained the decision trees on the training data set and then tested on train and test set.

```{r, echo=FALSE, eval=TRUE}
<<assign2-2>>
```

Below is the results using the deviance to measure the impurity. The first result is on the train set and the second on the test set.

```{r, echo=FALSE, eval=TRUE}
<<assign2-2-dtree-train>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-2-dtree-test>>
```


Below is the results using the Gini index to measure the impurity. The first result is on the train set and the second on the test set.

```{r, echo=FALSE, eval=TRUE}
<<assign2-2-gtree-train>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-2-gtree-test>>
```

We can see from the results that the deviance impurity measurement yield better on both the train and test data sets. In the following exercises I will therefore only be using that measure.

## 3
```{r, echo=FALSE, eval=TRUE}
<<assign2-3>>
```

Here I was interested in finding the optimal number of leaves in the tree and here I used the validation set to measure the performance. Figure \ref{fig:score} shows how the deviance varies with increasing number of leave nodes in the decision tree. We can see as the number of leaves increases the deviance decreases for the train set which indicate that the model starts overfitting the data. The deviance on the validation set tells a different story and the optimal number of leaves, i.e. yield the lowest deviance, is 4.

```{r score, echo=FALSE, eval=TRUE, fig.cap="Deviance score when adjusting the number of leaves (terminal nodes) in the decision tree."}
<<assign2-3-score>>
```

Given the optimal number of leaves being 4 from the validation set I pruned the tree to match that. Below are the predictions and misclassification rate on the test data set. It has increased the misclassification rate on the test set than the tree found previously but the complexity has been reduced significantly, from 15 leaves to 4.

```{r, echo=FALSE, eval=TRUE}
<<assign2-3-optimal>>
```

```{r tree, echo=FALSE, eval=TRUE, fig.cap="A visual representation of the optimal decision tree."}
<<assign2-3-plot>>
```

Figure \ref{fig:tree} shows the optimal tree with a depth of 3. We can see that having large savings are indicative of being good which is intuitively sound and if a customer have been a member of the company for a longer period of time it also indicates good loan management. I do not understand what the history variable means and can therefore draw any conclusions as to say if it make sense or not.

## 4
```{r, echo=FALSE, eval=TRUE}
<<assign2-4>>
```

Here I will be using the naive Bayes classifier instead and below are the results. The first result is on the train set and the second on the test set.

```{r, echo=FALSE, eval=TRUE}
<<assign2-4-train>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-4-test>>
```

The misclassification rate on the test data is far worse than the gotten from the optimal decision tree found in exercise 3.

## 5
```{r, echo=FALSE, eval=TRUE}
<<assign2-5>>
```

By changing the loss matrix, i.e. how much each error costs, the classification result will look differently. In the above example the cost was equally costly but here I have set that predicting good when the truth is bad, i.e. true negative, is 10 times as costly. Below are the results and the first result is on the train set and the second on the test set.

```{r, echo=FALSE, eval=TRUE}
<<assign2-5-train>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-5-test>>
```

What we can observe is that there are a lot of false negatives, i.e. predicting bad when the true outcome is good, because it is less costly to make that kind of error. The classifier has basically become scared of predicting good and this has in turn increased the misclassification rate significantly.

\newpage

# Appendix

## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>
<<assign1-1>>
<<assign1-2>>
<<assign1-3>>
<<assign1-4>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2-init>>
<<assign2-1>>

<<assign2-2>>
<<assign2-2-dtree-train>>
<<assign2-2-dtree-test>>
<<assign2-2-gtree-train>>
<<assign2-2-gtree-test>>

<<assign2-3>>
<<assign2-3-score>>
<<assign2-3-optimal>>
<<assign2-3-plot>>

<<assign2-4>>
<<assign2-4-train>>
<<assign2-4-test>>

<<assign2-5>>
<<assign2-5-train>>
<<assign2-5-test>>
```
