---
title: "Introduction to Machine Learning"
subtitle: "Lab 2 Block 2"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1b/solution.R")
knitr::read_chunk("../assignment2a/solution.R")
knitr::read_chunk("../assignment3a/solution.R")
knitr::read_chunk("../assignment4a/solution.R")
```

\newpage

# Assignment 1a
Assumptions:

\begin{gather*}
\mathrm{E} \left[ \epsilon^{b}(x) \right] = 0, \\
\forall_{i,j}, i \neq j: \mathrm{E} \left[ \epsilon^{i}(x) \epsilon^{j}(x) \right] = 0
\end{gather*}

Prove:

\begin{gather*}
\mathrm{E}_{X} \left[ (f_{\textit{bag}}(x) - h(x))^{2} \right] =
\frac{1}{B} \left[ \frac{1}{B} \sum_{b} \mathrm{E}_{X} \left[ (f^{b}(x) - h(x))^{2} \right] \right]
\end{gather*}

We know:

\begin{gather*}
f^{b}(x) = h(x) + \epsilon^{b}(x) \\
f_{\text{bag}}(x) = \frac{1}{B} \sum_{b} f^{b}(x)
\end{gather*}

Proof:

\begin{gather*}
\mathrm{E}_{X} \left[ (f_{\textit{bag}}(x) - h(x))^{2} \right] = \\
\mathrm{E}_{X} \left[ (\frac{1}{B} \sum_{b} f^{b}(x) - h(x))^{2} \right] = \\
\mathrm{E}_{X} \left[ (\frac{1}{B} \sum_{b} \epsilon^{b}(x))^{2} \right] = \\
\frac{1}{B^{2}}
\mathrm{E}_{X} \left[ (\epsilon^{1}(x))^{2} +
2 \epsilon^{1}(x) \epsilon^{2}(x) +
\dotsb +
2 \epsilon^{b-1}(x) \epsilon^{b}(x) +
(\epsilon^{b}(x))^{2} \right] = \\
\frac{1}{B^{2}}
\left(
\mathrm{E}_{X} \left[ (\epsilon^{1}(x))^{2} \right] +
2 \mathrm{E}_{X} \left[ \epsilon^{1}(x) \epsilon^{2}(x) \right]
+ \dotsb +
2 \mathrm{E}_{X} \left[ \epsilon^{b-1}(x) \epsilon^{b}(x) \right] +
\mathrm{E}_{X} \left[ (\epsilon^{b}(x))^{2} \right]
\right) = \\
\frac{1}{B^{2}} \sum_{b} \mathrm{E}_{X} \left[ (\epsilon^{b}(x))^{2} \right] = \\
\frac{1}{B^{2}} \sum_{b} \mathrm{E}_{X} \left[ (f^{b}(x) - h(x))^{2} \right] = \\
\frac{1}{B} \left[ \frac{1}{B} \sum_{b} \mathrm{E}_{X} \left[ (f^{b}(x) - h(x))^{2} \right] \right]
\end{gather*}

\newpage

# Assignment 2a
```{r, echo=FALSE, eval=TRUE}
<<assign2a-init>>

```

## 1
An estimated upperbound of the squared error of the bagging regression tree using 2/3 of the data set as training data and 1/3 for testing.

```{r, echo=TRUE, eval=TRUE}
<<assign2a-1-upperbound>>
```

## 2
An estimated upperbound of the squared error of the bagging regression tree using 3-fold cross-validation.

```{r, echo=TRUE, eval=TRUE}
<<assign2a-2-upperbound>>
```

## 3
The resulting bagging regression tree and its predicted value where data is the complete data set and newdata is the data to predict. I would return this result for both the techniques used to estimate the upperbound of the squared error.

```{r, echo=TRUE, eval=TRUE}
<<assign2a-3-bag>>
```

\newpage

# Assignment 3a
```{r, echo=FALSE, eval=TRUE}
<<assign3a-init>>
```

## 1
In boosting the number of iterations is a hyper-parameter and the plot below shows estimated squared errors using 10-fold cross-validation as the number of iterations increases from 1 to 100. The gray curves are the mean squared errors from each validation set and the black curve shows the mean of the those errors. The optimal seems to be around 35 iterations. By the elbow method we would rather choose around 15-19 iterations.

```{r, echo=FALSE, eval=TRUE}
<<assign3a-plot>>
```

## 2
Here I used 2/3 of the data for training and 1/3 as test data and created a boosting regression tree using the optimal number of iterations, i.e. the number of trees, found above. The squared errors for the two data sets were the following.

```{r, echo=FALSE, eval=TRUE}
<<assign3a-2>>
```

\newpage

# Assignment 4a
In this exercise I have used Adaboost classification trees and random forests to evauluate their performance on spam data. The data set have been divided into two parts, 2/3 for training and 1/3 as test data.

```{r, echo=FALSE, eval=TRUE}
<<assign4a-init>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign4a-tree>>
```

The performance of the Adaboost classification trees can be seen below. We can see that the optimal would be roughly 40 trees then it barely decreases as the number of trees grows. At 80 trees the test error seems to halt so if you want to push the limit and create a substantially more complex model it may be preferable to use 80 trees. However, since the error on the test data stops monotonically decrease after 40 trees that is the choice I would choose.

```{r, echo=FALSE, eval=TRUE}
<<assign4a-tree-plot>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign4a-forest>>
```

The performance of the random forest can be seen below. Same as above, the test error seem to stop monotonically decreasing after 40 trees and that should be the prefered choice. We can see that the train error barely moves as the number of trees increases after 20 trees so the model have almost fit the training data perfectly with 20 trees.

```{r, echo=FALSE, eval=TRUE}
<<assign4a-forest-plot>>
```

\newpage

# Assignment 1b
```{r, echo=TRUE, eval=TRUE, tidy=FALSE, highlight=TRUE}
<<assign1b-init>>
```

\newpage

# Assignment 2b
```{r, echo=FALSE, eval=TRUE}
<<assign1b-init>>
```

The plot below shows the three true multivariate Bernoulli distributions from which the data set have been generated.

```{r, echo=FALSE, eval=TRUE, fig.cap="The true probabilities of the multivariate Bernoulli distributions."}
<<assign1b-plot-truemu>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1b-EM-K2>>
```

The plot below shows two multivariate Bernoulli distributions estimated by the expectation-maximization (EM) algorithm. We can see that the multivariate Bernoulli with equal probabilities for each class has not affected EM particular much in order to find the other two distributions. This is probably because equal probabilities even out in the long run, i.e. the noise from that distribution is approximately equal for both sides of the coin.

```{r, echo=FALSE, eval=TRUE, fig.cap="The estimated probabilities of the multivariate Bernoulli distributions."}
<<assign1b-plot-estimatemu2>>
```

The prior probabilities for each distribution.

```{r, echo=FALSE, eval=TRUE}
<<assign1b-EM-estimatepi2>>
```

The plot below shows the log-likelihood versus the number of iterations.

```{r, echo=FALSE, eval=TRUE, fig.cap="The log-likelihood versus the number of iterations."}
<<assign1b-plot-llik2>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1b-EM-K3>>
```

The plot below shows three multivariate Bernoulli distributions estimated by the EM algorithm. The distributions found are pretty similar to the true ones with exceptation of the uniform one which have been influenced by the other two distributions and/or bad luck on the coin flips that have resulted in seemingly unfair coins.

```{r, echo=FALSE, eval=TRUE, fig.cap="The estimated probabilities of the multivariate Bernoulli distributions."}
<<assign1b-plot-estimatemu3>>
```

The prior probabilities for each distribution.

```{r, echo=FALSE, eval=TRUE}
<<assign1b-EM-estimatepi3>>
```

The plot below shows the log-likelihood versus the number of iterations.

```{r, echo=FALSE, eval=TRUE, fig.cap="The log-likelihood versus the number of iterations."}
<<assign1b-plot-llik3>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1b-EM-K4>>
```

The plot below shows four multivariate Bernoulli distributions estimated by the EM algorithm. The blue and red curves are quite chaotic that do not resemble any of the true ones but taking the average would approximate the multivariate Bernoulli distribution with uniform parameters pretty well. So the EM algorithm have basically modelled two distributions based on the noise from the uniform one which is not surprising given that there are only three true distributions and that one is the most unpredictable.

```{r, echo=FALSE, eval=TRUE, fig.cap="The estimated probabilities of the multivariate Bernoulli distributions."}
<<assign1b-plot-estimatemu4>>
```

The prior probabilities for each distribution.

```{r, echo=FALSE, eval=TRUE}
<<assign1b-EM-estimatepi4>>
```

The plot below shows the log-likelihood versus the number of iterations.

```{r, echo=FALSE, eval=TRUE, fig.cap="The log-likelihood versus the number of iterations."}
<<assign1b-plot-llik4>>
```

The maximum log-likelihood have been found after roughly 10 iterations for all experiments which indicates that it does not really matter in terms of iterations how many distributions the EM tries to find. However, the more distributions the more flucuations in the log-likelihood after each iterations which may suggest that the threshold should be a function of the number of distributions.

\newpage

# Appendix
## Code for Assignment 2a
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2a-init>>
<<assign2a-1-upperbound>>
<<assign2a-2-upperbound>>
<<assign2a-3-bag>>
```

## Code for Assignment 3a
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign3a-init>>
<<assign3a-plot>>
<<assign3a-2>>
```

## Code for Assignment 4a
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign4a-init>>
<<assign4a-tree>>
<<assign4a-tree-plot>>
<<assign4a-forest>>
<<assign4a-forest-plot>>
```

## Code for Assignment 2b
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1b-plot-truemu>>
<<assign1b-EM-K2>>
<<assign1b-plot-estimatemu2>>
<<assign1b-plot-llik2>>
<<assign1b-EM-K3>>
<<assign1b-plot-estimatemu3>>
<<assign1b-plot-llik3>>
<<assign1b-EM-K4>>
<<assign1b-plot-estimatemu4>>
<<assign1b-plot-llik4>>
```
