---
title: "Introduction to Machine Learning"
subtitle: "Lab 4"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
knitr::read_chunk("../assignment2/solution.R")
```

\newpage

# Assignment 1
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

## 1
```{r, echo=FALSE, eval=TRUE}
<<assign1-1>>
```

The data looks like it could be modelled reasonable by a cubic spline function.

## 2
```{r, echo=FALSE, eval=TRUE, results="hide"}
<<assign1-2>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-tree-plot>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-tree-fit>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-tree-resid>>
```

## 3
```{r, echo=FALSE, eval=TRUE, warning=FALSE}
<<assign1-3>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-confbounds>>
```

## 4
```{r, echo=FALSE, eval=TRUE, warning=FALSE}
<<assign1-4>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-4-confbounds>>
```

## 5

\newpage

# Assignment 2
```{r, echo=FALSE, eval=TRUE}
<<assign2-init>>
```

In this assignment I have used the NIRspectra data set that contains near-infrared spectra and viscosity levels for a collection of diesel fuels

## 1
```{r, echo=FALSE, eval=TRUE}
<<assign2-1>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-1-variance>>
```

```{r 21pcvar, echo=FALSE, eval=TRUE, fig.cap="Variance explained by the first 10 principal components."}
<<assign2-1-variance-plot>>
```

Figure \ref{fig:21pcvar} shows that basically all the variance can be explained by the first 5 principal components (PCs). The first two are clearly the most important and should be extracted without much loss of information, combined they explain over 99\% of the variance.

```{r 21score, echo=FALSE, eval=TRUE, fig.cap="The data projected onto the first two principal components."}
<<assign2-1-score>>
```

From figure \ref{fig:21score} there are clear outliers with very high values in the first PC. Most observations have a low value but rather spread out values in the second PC which is surprising since the first PC are supposed to explain most of the variance. However, since the scales are very different it make sense.

## 2
```{r, echo=FALSE, eval=TRUE}
<<assign2-2>>
```

```{r 22trace, echo=FALSE, eval=TRUE, fig.cap="Trace plot of the first two principal components."}
<<assign2-2-trace>>
```

From the trace plot in figure \ref{fig:22trace} we can observe that the first and second PCs are combinations of basically all the variables to different extent. The first PC has relatively high combination of all variables but slightly less of the last ones while the second PC is mostly a combination of the last ones.

## 3
```{r, echo=FALSE, eval=TRUE, warning=FALSE}
<<assign2-3>>
```

```{r 23trace, echo=FALSE, eval=TRUE, fig.cap="Traceplot of the first two independent components."}
<<assign2-3-trace>>
```

Similarly from the PC components in figure \ref{fig:22trace}, we can see that the independent components (ICs) in figure \ref{fig:23trace} have opposite shapes. The second IC is mostly a combination of the last variables while the first IC explains more information from the other variables.

```{r 23score, echo=FALSE, eval=TRUE, fig.cap="The data projected onto the first two independent components."}
<<assign2-3-score>>
```

The scores in figure \ref{fig:23score} have similar shape to that of the principal components (fig. \ref{fig:21score}) but with very different scales. Outliers can clearly be detected in the data.

## 4
```{r, echo=FALSE, eval=TRUE}
<<assign2-4>>
```

In this exercise I have used principal component regression (PCR) in order to estimate the viscosity.

```{r 24msep, echo=FALSE, eval=TRUE, fig.cap="Mean Squared Error of Prediction against number of principal components."}
<<assign2-4-MSEP>>
```

Figure \ref{fig:24msep} shows how the mean squared error changes when varying the number of PCs in PCR using cross validation. The optimal number of PCs are probably either around 7 or 17 depending on how much you value fewer features and a simpler model.

\newpage

# Appendix

## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>
<<assign1-1>>

<<assign1-2>>
<<assign1-2-tree-plot>>
<<assign1-2-tree-fit>>
<<assign1-2-tree-resid>>

<<assign1-3>>
<<assign1-3-confbounds>>

<<assign1-4>>
<<assign1-4-confbounds>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2-init>>

<<assign2-1>>
<<assign2-1-variance>>
<<assign2-1-variance-plot>>
<<assign2-score>>

<<assign2-2>>
<<assign2-2-trace>>

<<assign2-3>>
<<assign2-3-trace>>
<<assign2-3-score>>

<<assign2-4>>
<<assign2-4-MSEP>>
```
