---
title: "Introduction to Machine Learning"
subtitle: "Lab 3 Block 2"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
knitr::read_chunk("../assignment2/solution.R")
```

\newpage

# Assignment 1
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

In this assignment I have used the data data set that contains 64 e-mails which were manually collected from DBWorld mailing list. Each e-mail consists of 4702 features, i.e. unique words in the e-mails, and I want to predict whether the e-mail is a conference e-mail or something else. In order to do so I have divided the data set into train and test sets corresponding to 70\% respective 30\% of the data.

## 1
```{r, echo=FALSE, eval=TRUE, results="hide"}
<<assign1-1-nsc>>
```

Here I performed neastest shrunken centroid on the training set where the threshold was chosen by 10-fold cross-validation. Below are the results and we can see that the optimal threshold was `r nsc_optimal_threshold` resulting in `r nsc_optimal_size` non-zero features and a classification error of `r nsc_class_error * 100`\%. We can see that words like \textit{papers}, \textit{submission}, \textit{published}, and \textit{conference} are important words in order to classify an e-mail as a conference e-mail which seem reasonable.

```{r, echo=FALSE, eval=TRUE}
<<assign1-1-nsc-result>>
```

Figure \ref{fig:nsc} is the resulting centroid plot that shows the contributions of the words to each class where a conference e-mail is a 1. We can see that the words have opposite contributions to the classes, i.e. if the contribution is high for class 1 then it is low for class 0.

```{r nsc, echo=FALSE, eval=TRUE, fig.cap="Centroid plot."}
<<assign1-1-nsc-plot>>
```

## 2
### Elastic Net
```{r, echo=FALSE, eval=TRUE}
<<assign1-2-elasticnet>>
```

Elastic net is another method for feature extraction and I let cross-validation choose the penalty measurement. In this can it can be shown below that it picked \textit{deviance} which results in 38 non-zero features and a misclassification rate of 15\%.

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-elasticnet-result>>
```

### Support Vector Machine
```{r, echo=FALSE, eval=TRUE, results="hide"}
<<assign1-2-svm>>
```

Lastly I used the support vector machine and here the size correspond to the number of support vectors rather than features, i.e. all features are used but against a subset of training samples. The number of support vectors was 43 with a misclassification rate of 5\% which compared to elastic net is very good.

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-svm-result>>
```

From the summary below we can see that nearest chrunken centroid and support vector machine are the best in terms of classification error. \textbf{TODO}: Which one is better? Fewer observations to check in support vector but only 43 / 64 which is not that much. The number of features are still the same while in nsc the features are reduced by half. Is elastic net really that terrible considering the number of features?

\begin{center}
```{r, echo=FALSE, eval=TRUE}
<<assign1-2-summary>>
```
\end{center}

## 3
```{r, echo=FALSE, eval=TRUE}
<<assign1-3-benjhoch>>
```

In total there were 39 rejected hypothesis by the Benjamini-Hochberg algorithm using $\alpha = 0.05$ and the ten most significant features found are quite similar to those found by the nearest chrunken centroid and make total sense as seen below.

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-features>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-plot>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-plot-zoom>>
```

\newpage

# Assignment 2
```{r, echo=FALSE, eval=TRUE}
<<assign2-init>>

```
In this assignment I have implemented a version of the budget online support vector machine (BOSVM) and it has parameters controlling the maximum number of support vectors that should be used in the prediction denoted $M$ and the fault tolerance $\beta$. A faulty prediction means that the sign of the prediction does not correspond with the sign of the true label. I ran the BOSVM for 500 iterations, i.e. samples, with four different settings which can be seen below with the corresponding number of final support vectors in the model.

```{r, echo=FALSE, eval=TRUE}
<<assign2-run1>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run2>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run3>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run4>>
```
Figure \ref{fig:svm_error} shows how the error rate differ when varying the number of maximum support vectors and the fault tolerance. We can see that setting $\beta = 0$ is better in both cases which does make sense since we will adjust our model every misclassification and for the case where $M = 500$ it will start overfit the data. Using a $\beta < 0$ means we accept some misclassification without adjusting our model and this result in slightly higher error rate but with far fewer support vectors as we can see above when $M = 500$, 48 compared to 106.

```{r svm_error, echo=FALSE, eval=TRUE, fig.cap="The error rate for different $\\beta$ and maximum number of support vectors, M. B0 means $\\beta = 0$ and B5 means $\\beta = -0.05$."}
<<assign2-plot>>
```

So what we can say is that when using $M = 500$ the $\beta$ controls how much the model fits the data and therefore using $\beta = 0$ are more prone to overfitting than $\beta = -0.05$ and gets a lower error rate. BOSVM with $M = 20$ and $\beta = -0.05$ has a slightly smoother error curve than with $\beta = 0$ because it does not change the support vectors for every faulty prediction and so it basically stabilizes and get a subset of more general support vectors that have greater predictive power. One problem with using $M = 20$ and $\beta = 0$ is that we have use low amounts of support vectors and every misclassification adds a new support vector which means that the calculation for removing a current support vector is executed many times and it is the most expensive operation in the algorithm hence slows down the execution time.

\newpage

# Appendix
## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>

<<assign1-1-nsc>>
<<assign1-1-nsc-result>>
<<assign1-1-nsc-plot>>

<<assign1-2-elasticnet>>
<<assign1-2-elasticnet-result>>
<<assign1-2-svm>>
<<assign1-2-svm-result>>
<<assign1-2-summary>>

<<assign1-3-benjhoch>>
<<assign1-3-features>>
<<assign1-3-plot>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2-init>>
<<assign2-run1>>
<<assign2-run2>>
<<assign2-run3>>
<<assign2-run4>>
<<assign2-plot>>
<<assign1-3-plot-zoom>>
```
