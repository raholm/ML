---
title: "Introduction to Machine Learning"
subtitle: "Lab 3 Block 2"
author: "Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
knitr::read_chunk("../assignment2/solution.R")
```

\newpage

# Assignment 1
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

In this assignment I have used the data data set that contains 64 e-mails which were manually collected from DBWorld mailing list. Each e-mail consists of 4702 features, i.e. unique words in the e-mails, and I want to predict whether the e-mail is a conference e-mail or something else. In order to do so I have divided the data set into train and test sets corresponding to 70\% respective 30\% of the data.

## 1
```{r, echo=FALSE, eval=TRUE, results="hide"}
<<assign1-1-nsc>>
```

Here I performed nearest shrunken centroid on the training set where the threshold was chosen by 10-fold cross-validation. Below are the results and we can see that the optimal threshold was `r nsc_optimal_threshold` resulting in `r nsc_optimal_size` non-zero features and a classification error of `r nsc_class_error * 100`\%. We can see that words like \textit{papers}, \textit{submission}, \textit{published}, and \textit{conference} are important words in order to classify an e-mail as a conference e-mail which seem reasonable.

```{r, echo=FALSE, eval=TRUE}
<<assign1-1-nsc-result>>
```

Figure \ref{fig:nsc} is the resulting centroid plot that shows the contributions of the words to each class where a conference e-mail is a 1. We can see that the words have opposite contributions to the classes, i.e. if the contribution is high for class 1 then it is low for class 0.

```{r nsc, echo=FALSE, eval=TRUE, fig.cap="Centroid plot."}
<<assign1-1-nsc-plot>>
```

## 2
### Elastic Net
```{r, echo=FALSE, eval=TRUE}
<<assign1-2-elasticnet>>
```

Elastic net is another method for feature extraction and I let cross-validation choose the penalty measurement. In this can it can be shown below that it picked \textit{`r en_penalty`} which results in `r en_optimal_size` non-zero features and a misclassification rate of `r en_class_error * 100`\%.

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-elasticnet-result>>
```

### Support Vector Machine
```{r, echo=FALSE, eval=TRUE, results="hide"}
<<assign1-2-svm>>
```

Lastly I used the support vector machine and here the size correspond to the number of support vectors rather than features, i.e. all features are used but against a subset of training samples. The number of support vectors was `r svm_optimal_size` with a misclassification rate of `r svm_class_error * 100`\% which compared to elastic net is very good.

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-svm-result>>
```

From a classification error point of view in the table below the support vector is better than both elastic net and nearest shrunken centroid, but in this case we only have 20 observations in the test set which means +5\% error rate is just an additional misclassification. Given the information we have I would prefer the nearest shrunken centroid since it reduces the feature space from 4702 to just 12 features with an additional misclassification compared to the support vector machine which uses all features. The elastic net have almost 3 times the number of features as nearest shrunken centroid with an additional misclassification so it is clearly a worse model with an increased complexity.

\begin{center}
```{r, echo=FALSE, eval=TRUE}
<<assign1-2-summary>>
```
\end{center}

## 3
```{r, echo=FALSE, eval=TRUE}
<<assign1-3-benjhoch>>
```

In total there were 39 rejected hypothesis by the Benjamini-Hochberg algorithm using $\alpha = 0.05$ and the ten most significant features found are quite similar to those found by the nearest shrunken centroid and make total sense as seen below.

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-features>>
```

Below are plots of the p-values for each feature in sorted order and the second plot just shows the lowest 150 p-values. The red points are those features where the null hypotheses was rejected, i.e. those are statistically significant in explaining the \textit{Conference} variable given $\alpha = 0.05$.

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-plot>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-plot-zoom>>
```

\newpage

# Assignment 2
```{r, echo=FALSE, eval=TRUE}
<<assign2-init>>

```
In this assignment I have implemented a version of the budget online support vector machine (BOSVM) and it has parameters controlling the maximum number of support vectors that should be used in the prediction denoted $M$ and the fault tolerance $\beta$. A faulty prediction means that the sign of the prediction does not correspond with the sign of the true label. I ran the BOSVM for 500 iterations, i.e. samples, with four different settings which can be seen below with the corresponding number of final support vectors in the model.

```{r, echo=FALSE, eval=TRUE}
<<assign2-run1>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run2>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run3>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run4>>
```
Figure \ref{fig:svm_error} shows how the error rate differ when varying the number of maximum support vectors and the fault tolerance. We can see that setting $\beta = 0$ is better in both cases which does make sense since we will adjust our model every misclassification and for the case where $M = 500$ it will start overfit the data. Using a $\beta < 0$ means we accept some misclassification without adjusting our model and this result in slightly higher error rate but with far fewer support vectors as we can see above when $M = 500$, 48 compared to 106.

```{r svm_error, echo=FALSE, eval=TRUE, fig.cap="The error rate for different $\\beta$ and maximum number of support vectors, M. B0 means $\\beta = 0$ and B5 means $\\beta = -0.05$."}
<<assign2-plot>>
```

So what we can say is that when using $M = 500$ the $\beta$ controls how much the model fits the data and therefore using $\beta = 0$ are more prone to overfitting than $\beta = -0.05$ and gets a lower error rate. BOSVM with $M = 20$ and $\beta = -0.05$ has a slightly smoother error curve than with $\beta = 0$ because it does not change the support vectors for every faulty prediction and so it basically stabilizes and get a subset of more general support vectors that have greater predictive power. One problem with using $M = 20$ and $\beta = 0$ is that we have set the total amount of support vectors to a low value and every misclassification adds a new support vector which means that the calculation for removing a current support vector is executed many times and it is the most expensive operation in the algorithm hence slows down the execution time significantly.

\newpage

# Appendix
## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>

<<assign1-1-nsc>>
<<assign1-1-nsc-result>>
<<assign1-1-nsc-plot>>

<<assign1-2-elasticnet>>
<<assign1-2-elasticnet-result>>
<<assign1-2-svm>>
<<assign1-2-svm-result>>
<<assign1-2-summary>>

<<assign1-3-benjhoch>>
<<assign1-3-features>>
<<assign1-3-plot>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2-init>>
<<assign2-run1>>
<<assign2-run2>>
<<assign2-run3>>
<<assign2-run4>>
<<assign2-plot>>
<<assign1-3-plot-zoom>>
```
