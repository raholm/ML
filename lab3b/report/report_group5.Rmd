---
title: "Introduction to Machine Learning"
subtitle: "Lab 3 Block 2"
author: "Anton Persson, Emil Klasson Svensson, Mattias Karlsson, Rasmus Holm"
date: "`r Sys.Date()`"
fontsize: 10pt
geometry: margin=1in
output:
    pdf_document:
        toc: true
        number_sections: false
        fig_caption: yes
        keep_tex: yes
        includes:
            in_header: styles.sty
---

```{r global-options, echo = FALSE, eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
knitr::opts_chunk$set(fig.pos='H', fig.align='center')

knitr::read_chunk("../assignment1/solution.R")
knitr::read_chunk("../assignment2/solution.R")
```

\newpage

# Assignment 1
```{r, echo=FALSE, eval=TRUE}
<<assign1-init>>
```

In this assignment we have used the data.csv data set that contains 64 e-mails which were manually collected from DBWorld mailing list. Each e-mail consists of 4702 features, i.e. unique words in the e-mails, and we want to predict whether the e-mail is a conference e-mail or something else. In order to do so we divided the data set into train and test sets corresponding to 70\% (44 obs.) respective 30\% (20 obs.) of the data.

## 1
```{r, echo=FALSE, eval=TRUE, results="hide"}
<<assign1-1-nsc>>
```

In this first part we are supposed to use the nearest shrunken centroid method on the training set where the threshold is chosen by 10-fold cross-validation. Below are the results and we can see that the optimal threshold was `r nsc_optimal_threshold` resulting in `r nsc_optimal_size` non-zero features and a classification error of `r nsc_class_error * 100`\%. The words in the list of the top 10 most contributing ones are reasonable to discriminate conference e-mails from other e-mails.

```{r, echo=FALSE, eval=TRUE}
<<assign1-1-nsc-result>>
```

Figure \ref{fig:nsc} is the resulting centroid plot that shows the contributions of the words to each class where a conference e-mail is a 1. We can see that the words have opposite contributions to the classes, i.e. if the contribution is high for class 1 then it is low for class 0.

```{r nsc, echo=FALSE, eval=TRUE, fig.cap="Centroid plot."}
<<assign1-1-nsc-plot>>
```

## 2
### Elastic Net
```{r, echo=FALSE, eval=TRUE}
<<assign1-2-elasticnet>>
```

Elastic net is another method for feature extraction and we have let cross-validation choose the penalty measurement for us. In this can it can be shown below that it picked \textit{`r en_penalty`} which results in `r en_optimal_size` non-zero features and a misclassification rate of `r en_class_error * 100`\%.

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-elasticnet-result>>
```

### Support Vector Machine
```{r, echo=FALSE, eval=TRUE, results="hide"}
<<assign1-2-svm>>
```

Lastly, we used the support vector machine and here the size correspond to the number of support vectors rather than features, i.e. all features are used but against a subset of training samples. The number of support vectors was `r svm_optimal_size` with a misclassification rate of `r svm_class_error * 100`\% which compared to elastic net is very good.

```{r, echo=FALSE, eval=TRUE}
<<assign1-2-svm-result>>
```

From a classification error point of view in the table below the support vector is better than both elastic net and nearest shrunken centroid, but in this case we only have 20 observations in the test set which means +5\% error rate is just an additional misclassification. Given the information we have we would prefer the nearest shrunken centroid since it reduces the feature space from 4702 to just `r nsc_optimal_size` features with an additional misclassification compared to the support vector machine which uses all features. The elastic net have almost 3 times the number of features as nearest shrunken centroid with an additional misclassification so it is clearly a worse model both in terms of complexity and error rate.

\begin{center}
```{r, echo=FALSE, eval=TRUE}
<<assign1-2-summary>>
```
\end{center}

## 3
```{r, echo=FALSE, eval=TRUE}
<<assign1-3-benjhoch>>
```

In total there were `r rejected` rejected hypothesis by the Benjamini-Hochberg algorithm using $\alpha = 0.05$ and the ten most significant features found are quite similar to those found by the nearest chrunken centroid and make total sense as seen below.

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-features>>
```

Below are plots of the p-values for each feature in sorted order and the second plot just shows the lowest 150 p-values. The red points are those features where the null hypotheses was rejected, i.e. those are statistically significant in explaining the \textit{Conference} variable given $\alpha = 0.05$.

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-plot>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign1-3-plot-zoom>>
```

\newpage

# Assignment 2
```{r, echo=FALSE, eval=TRUE}
<<assign2-init>>
```

In this assignment we implemented a version of the budget online support vector machine (BOSVM) and it has parameters controlling the maximum number of support vectors that should be used in the prediction denoted $M$ and the fault tolerance $\beta$. A faulty prediction means that the sign of the prediction does not correspond with the sign of the true label assuming a binary classification $\left \{ -1, 1 \right \}$. We ran the BOSVM for 500 iterations, i.e. samples, with four different settings which can be seen below with the corresponding number of final support vectors in the model.

```{r, echo=FALSE, eval=TRUE}
<<assign2-run1>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run2>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run3>>
```

```{r, echo=FALSE, eval=TRUE}
<<assign2-run4>>
```

Figure \ref{fig:svm_error} shows how the error rate differ for the various settings. We can see that setting $\beta = 0$ is better in both cases which does make sense since we will adjust our model every misclassification and for the case where $M = 500$ it will start overfit the data. Using a $\beta < 0$ means we accept some misclassification without adjusting our model and this result in slightly higher error rate but with far fewer support vectors as we can see above when $M = 500$, 48 compared to 106.

```{r svm_error, echo=FALSE, eval=TRUE, fig.cap="The error rate for different $\\beta$ and maximum number of support vectors, M. B0 means $\\beta = 0$ and B5 means $\\beta = -0.05$."}
<<assign2-plot>>
```

So what we can say is that when using $M = 500$ the $\beta$ controls how much the model fits the data and therefore using $\beta = 0$ are more prone to overfitting than $\beta = -0.05$ and gets a lower error rate. BOSVM with $M = 20$ and $\beta = -0.05$ has a slightly smoother error curve than with $\beta = 0$ because it does not change the support vectors for every faulty prediction. This is due to stabilization and the set of support vectors are more general and have greater predictive power. One problem with using $M = 20$ and $\beta = 0$ is that we have set the total amount of support vectors to a low value and every misclassification adds a new support vector which means that the calculation for removing a current support vector is executed many times and it is the most expensive operation in the algorithm hence slows down the execution time significantly.

\newpage

# Appendix
## Code for Assignment 1
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign1-init>>

<<assign1-1-nsc>>
<<assign1-1-nsc-result>>
<<assign1-1-nsc-plot>>

<<assign1-2-elasticnet>>
<<assign1-2-elasticnet-result>>
<<assign1-2-svm>>
<<assign1-2-svm-result>>
<<assign1-2-summary>>

<<assign1-3-benjhoch>>
<<assign1-3-features>>
<<assign1-3-plot>>
```

## Code for Assignment 2
```{r, echo=TRUE, eval=FALSE, tidy=FALSE, highlight=TRUE}
<<assign2-init>>
<<assign2-run1>>
<<assign2-run2>>
<<assign2-run3>>
<<assign2-run4>>
<<assign2-plot>>
<<assign1-3-plot-zoom>>
```

## Contributions
We divided the work into two parts and discussed/compiled the results in pairs. Then we all discussed our findings together as a whole group and checked that everyone had similar/understood the results.
